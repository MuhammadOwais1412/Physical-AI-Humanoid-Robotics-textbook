---
id: 001
title: Generate Physical AI Textbook Specification
stage: spec
date: 2025-12-05
surface: agent
model: claude-sonnet-4-5-20250929
feature: textbook-creation
branch: master
user: user
command: "{Project Details: \"{Create a Textbook for Teaching Physical AI & Humanoid Robotics Course:...}\"}\n\n---\n\nYour task is to generate a complete, clear, and structured **Specification Highlights** document. ..."
labels: [specification, physical-ai, robotics, education]
links:
  spec: specs/textbook-creation/spec.md
  ticket: null
  adr: null
  pr: null
files:
  - specs/textbook-creation/spec.md
tests: null
---

## Prompt

```
{Project Details: "{Create a Textbook for Teaching Physical AI & Humanoid Robotics Course:\nThe Course Details\nTitle: \"**Physical AI & Humanoid Robotics**\"\nFocus and Theme: AI Systems in the Physical World. Embodied Intelligence.\nGoal: Bridging the gap between the digital brain and the physical body. Students apply their AI knowledge to control Humanoid Robots in simulated and real-world environments.\nQuarter Overview\nThe future of AI extends beyond digital spaces into the physical world. This capstone quarter introduces Physical AI‚ÄîAI systems that function in reality and comprehend physical laws. Students learn to design, simulate, and deploy humanoid robots capable of natural human interactions using ROS 2, Gazebo, and NVIDIA Isaac.\nModule 1: The Robotic Nervous System (ROS 2)\nFocus: Middleware for robot control.\nROS 2 Nodes, Topics, and Services.\nBridging Python Agents to ROS controllers using rclpy.\nUnderstanding URDF (Unified Robot Description Format) for humanoids.\n\n\nModule 2: The Digital Twin (Gazebo & Unity)\nFocus: Physics simulation and environment building.\nSimulating physics, gravity, and collisions in Gazebo.\nHigh-fidelity rendering and human-robot interaction in Unity.\nSimulating sensors: LiDAR, Depth Cameras, and IMUs.\n\n\nModule 3: The AI-Robot Brain (NVIDIA Isaac‚Ñ¢)\nFocus: Advanced perception and training.\nNVIDIA Isaac Sim: Photorealistic simulation and synthetic data generation.\nIsaac ROS: Hardware-accelerated VSLAM (Visual SLAM) and navigation.\nNav2: Path planning for bipedal humanoid movement.\n\n\nModule 4: Vision-Language-Action (VLA)\nFocus: The convergence of LLMs and Robotics.    \nVoice-to-Action: s\nWeekly Breakdown\nWeeks 1-2: Introduction to Physical AI\nFoundations of Physical AI and embodied intelligence\nFrom digital AI to robots that understand physical laws\nOverview of humanoid robotics landscape\nSensor systems: LIDAR, cameras, IMUs, force/torque sensors\nWeeks 3-5: ROS 2 Fundamentals\nROS 2 architecture and core concepts\nNodes, topics, services, and actions\nBuilding ROS 2 packages with Python\nLaunch files and parameter management\nWeeks 6-7: Robot Simulation with Gazebo\nGazebo simulation environment setup\nURDF and SDF robot description formats\nPhysics simulation and sensor simulation\nIntroduction to Unity for robot visualization\nWeeks 8-10: NVIDIA Isaac Platform\nNVIDIA Isaac SDK and Isaac Sim\nAI-powered percng OpenAI Whisper for voice commands.\nCognitive Planning: Using LLMs to translate natural language (\"Clean the room\") into a sequence of ROS 2 actions.\nCapstone Project: The Autonomous Humanoid. A final project where a simulated robot receives a voice command, plans a path, navigates obstacles, identifies an object using computer vision, and manipulates it.\n\nWhy Physical AI Matters\nHumanoid robots are poised to excel in our human-centered world because they share our physical form and can be trained with abundant data from interacting in human environments. This represents a significant transition from AI models confined to digital environments to embodied intelligence that operates in physical space.\nLearning Outcomes\nUnderstand Physical AI principles and embodied intelligence\nMaster ROS 2 (Robot Operating System) for robotic control\nSimulate robots with Gazebo and Unity\nDevelop with NVIDIA Isaac AI robot platform\nDesign humanoid robots for natural interactions\nIntegrate GPT models for conversational robotiception and manipulation\nReinforcement learning for robot control\nSim-to-real transfer techniques\nWeeks 11-12: Humanoid Robot Development\nHumanoid robot kinematics and dynamics\nBipedal locomotion and balance control\nManipulation and grasping with humanoid hands\nNatural human-robot interaction design\n\nWeek 13: Conversational Robotics\nIntegrating GPT models for conversational AI in robots\nSpeech recognition and natural language understanding\nMulti-modal interaction: speech, gesture, vision\nAssessments\nROS 2 package development project\nGazebo simulation implementation\nIsaac-based perception pipeline\nCapstone: Simulated humanoid robot with conversational AI\n\nHardware Requirements\nThis course is technically demanding. It sits at the intersection of three heavy computational loads: Physics Simulation (Isaac Sim/Gazebo), Visual Perception (SLAM/Computer Vision), and Generative AI (LLMs/VLA).\n\nBecause the capstone involves a \"Simulated Humanoid,\" the primary investment must be in High-Performance Workstations. However, to fulfill the \"Physical AI\" promise, you also need Edge Computing Kits (brains without bodies) or specific robot hardware.\n\n1. The \"Digital Twin\" Workstation (Required per Student)\nThis is the most critical component. NVIDIA Isaac Sim is an Omniverse application that requires \"RTX\" (Ray Tracing) capabilities. Standard laptops (MacBooks or non-RTX Windows machines) will not work.\nGPU (The Bottleneck): NVIDIA RTX 4070 Ti (12GB VRAM) or higher.\nWhy: You need high VRAM to load the USD (Universal Scene Description) assets for the robot and environment, plus run the VLA (Vision-Language-Action) models simultaneously.\nIdeal: RTX 3090 or 4090 (24GB VRAM) allows for smoother \"Sim-to-Real\" training.\nCPU: Intel Core i7 (13th Gen+) or AMD Ryzen 9.\nWhy: Physics calculations (Rigid Body Dynamics) in Gazebo/Isaac are CPU-intensive.\nRAM: 64 GB DDR5 (32 GB is the absolute minimum, but will crash during complex scene rendering).\nOS: Ubuntu 22.04 LTS.\nNote: While Isaac Sim runs on Windows, ROS 2 (Humble/Iron) is native to Linux. Dual-booting or dedicated Linux machines are mandatory for a friction-free experience.\n\n\n2. The \"Physical AI\" Edge Kit\nSince a full humanoid robot is expensive, students learn \"Physical AI\" by setting up the nervous system on a desk before deploying it to a robot. This kit covers Module 3 (Isaac ROS) and Module 4 (VLA).\nThe Brain: NVIDIA Jetson Orin Nano (8GB) or Orin NX (16GB).\nRole: This is the industry standard for embodied AI. Students will deploy their ROS 2 nodes here to understand resource constraints vs. their powerful workstations.\nThe Eyes (Vision): Intel RealSense D435i or D455.\nRole: Provides RGB (Color) and Depth (Distance) data. Essential for the VSLAM and Perception modules.\nThe Inner Ear (Balance): Generic USB IMU (BNO055) (Often built into the RealSense D435i or Jetson boards, but a separate module helps teach IMU calibration).\nVoice Interface: A simple USB Microphone/Speaker array (e.g., ReSpeaker) for the \"Voice-to-Action\" Whisper integration.\n3. The Robot Lab\nFor the \"Physical\" part of the course, you have three tiers of options depending on budget.\nOption A: The \"Proxy\" Approach (Recommended for Budget)\nUse a quadruped (dog) or a robotic arm as a proxy. The software principles (ROS 2, VSLAM, Isaac Sim) transfer 90% effectively to humanoids.\nRobot: Unitree Go2 Edu (~$1,800 - $3,000).\nPros: Highly durable, excellent ROS 2 support, affordable enough to have multiple units.\nCons: Not a biped (humanoid).\nOption B: The \"Miniature Humanoid\" Approach\nSmall, table-top humanoids.\nRobot: Unitree H1 is too expensive ($90k+), so look at Unitree G1 (~$16k) or Robotis OP3 (older, but stable, ~$12k).\nBudget Alternative: Hiwonder TonyPi Pro (~$600).\nWarning: The cheap kits (Hiwonder) usually run on Raspberry Pi, which cannot run NVIDIA Isaac ROS efficiently. You would use these only for kinematics (walking) and use the Jetson kits for AI.\nOption C: The \"Premium\" Lab (Sim-to-Real specific)\nIf the goal is to actually deploy the Capstone to a real humanoid:\nRobot: Unitree G1 Humanoid.\nWhy: It is one of the few commercially available humanoids that can actually walk dynamically and has an SDK open enough for students to inject their own ROS 2 controllers.\n\n4. Summary of Architecture\nTo teach this successfully, your lab infrastructure should look like this:\nComponent\nHardware\nFunction\nSim Rig\nPC with RTX 4080 + Ubuntu 22.04\nRuns Isaac Sim, Gazebo, Unity, and trains LLM/VLA models.\nEdge Brain\nJetson Orin Nano\nRuns the \"Inference\" stack. Students deploy their code here.\nSensors\nRealSense Camera + Lidar\nConnected to the Jetson to feed real-world data to the AI.\nActuator\nUnitree Go2 or G1 (Shared)\nReceives motor commands from the Jetson.\n\n\nIf you do not have access to RTX-enabled workstations, we must restructure the course to rely entirely on cloud-based instances (like AWS RoboMaker or NVIDIA's cloud delivery for Omniverse), though this introduces significant latency and cost complexity.\n\nBuilding a \"Physical AI\" lab is a significant investment. You will have to choose between building a physical On-Premise Lab at Home (High CapEx) versus running a Cloud-Native Lab (High OpEx).\n\nOption 2 High OpEx: The \"Ether\" Lab (Cloud-Native)\nBest for: Rapid deployment, or students with weak laptops.\n1. Cloud Workstations (AWS/Azure) Instead of buying PCs, you rent instances.\nInstance Type: AWS g5.2xlarge (A10G GPU, 24GB VRAM) or g6e.xlarge.\nSoftware: NVIDIA Isaac Sim on Omniverse Cloud (requires specific AMI).\nCost Calculation:\nInstance cost: ~$1.50/hour (spot/on-demand mix).\nUsage: 10 hours/week √ó 12 weeks = 120 hours.\nStorage (EBS volumes for saving environments): ~$25/quarter.\nTotal Cloud Bill: ~$205 per quarter.\n2. Local \"Bridge\" Hardware You cannot eliminate hardware entirely for \"Physical AI.\" You still need the edge devices to deploy the code physically.\nEdge AI Kits: You still need the Jetson Kit for the physical deployment phase.\nCost: $700 (One-time purchase).\nRobot: You still need one physical robot for the final demo.\nCost: $3,000 (Unitree Go2 Standard).\nThe Economy Jetson Student Kit\nBest for: Learning ROS 2, Basic Computer Vision, and Sim-to-Real control.\nComponent\nModel\nPrice (Approx.)\nNotes\nThe Brain\nNVIDIA Jetson Orin Nano Super Dev Kit (8GB)\n$249\nNew official MSRP (Price dropped from ~$499). Capable of 40 TOPS.\nThe Eyes\nIntel RealSense D435i\n$349\nIncludes IMU (essential for SLAM). Do not buy the D435 (non-i).\nThe Ears\nReSpeaker USB Mic Array v2.0\n$69\nFar-field microphone for voice commands (Module 4).\nWi-Fi\n(Included in Dev Kit)\n$0\nThe new \"Super\" kit includes the Wi-Fi module pre-installed.\nPower/Misc\nSD Card (128GB) + Jumper Wires\n$30\nHigh-endurance microSD card required for the OS.\nTOTAL\n\n\n~$700 per kit\n\n\n\n3. The Latency Trap (Hidden Cost)\nSimulating in the cloud works well, but controlling a real robot from a cloud instance is dangerous due to latency.\nSolution: Students train in the Cloud, download the model (weights), and flash it to the local Jetson kit.\n\n}"\n}\n\n---\n\nYour task is to generate a complete, clear, and structured **Specification Highlights** document.  \nThe spec must follow Spec-Kit Plus standards and include the following sections:\n\n------------------------------------------------------------\n1. TARGET AUDIENCE\nIdentify who the project is for. Describe:\n- Primary users\n- Stakeholders\n- Skill level or background\n- Any relevant demographics or usage contexts\n\n------------------------------------------------------------\n2. SUCCESS CRITERIA\nDefine what success looks like in measurable, testable terms:\n- Functional outcomes\n- User experience expectations\n- Performance or reliability indicators\n- Completion or delivery milestones\n\n------------------------------------------------------------\n3. CONSTRAINTS\nList all boundaries and limitations that guide the project:\n- Technical constraints\n- Toolchain requirements (e.g., Docusaurus, GitHub Pages, Spec-Kit Plus, Claude Code)\n- Scope limitations\n- Time, resources, or platform restrictions\n- Non-negotiable compliance rules\n\n------------------------------------------------------------\n4. NOT BUILDING (NON-GOALS)\nClearly state what is *explicitly outside* the project scope:\n- Features intentionally excluded\n- Adjacent problems or areas not addressed\n- Misinterpretations or expectations to avoid\n- Anti-goals that keep the project focused\n\n------------------------------------------------------------\n5. INTENT SUMMARY (Automatically derived)\nSynthesize the above information to produce:\n- What you are building\n- Why you are building it\n- The core direction and purpose of the project\n\nThis section should be crisp, concise, and aligned with the project‚Äôs vision.\n\n------------------------------------------------------------\n6. STRUCTURED OUTPUT FORMAT\nProduce the final result in the following order:\n\n### **Intent**\n(Short purpose statement)\n\n### **Target Audience**\n(Bulleted list or short paragraphs)\n\n### **Success Criteria**\n(Measurable acceptance criteria)\n\n### **Constraints**\n(Boundaries, limitations, must-follow rules)\n\n### **Non-Goals**\n(What will NOT be built)\n\n------------------------------------------------------------\nREQUIREMENTS:\n- Ensure clarity, precision, and no contradictions.\n- Keep the spec concise but complete.\n- Align with Spec-Kit Plus conventions for specification writing.\n- Avoid ambiguity and maintain compatibility with all project requirements.\n---Usi
```

## Response snapshot

Created the 'Specification Highlights' document for the 'Physical AI & Humanoid Robotics' course, detailing target audience, success criteria, constraints, and non-goals. Document saved to `specs/textbook-creation/spec.md`.

## Outcome

- ‚úÖ Impact: Specification for textbook generation successfully created and saved.
- üß™ Tests: No tests were run for this specification generation task.
- üìÅ Files: One file (`specs/textbook-creation/spec.md`) was created.
- üîÅ Next prompts: Next steps could involve generating a plan (`/sp.plan`) or tasks (`/sp.tasks`) based on this specification.
- üß† Reflection: The task involved extracting structured information from a detailed project description to create a formal specification document. The process was straightforward, and the output adheres to the requested format and content requirements.

## Evaluation notes (flywheel)

- Failure modes observed: None.
- Graders run and results (PASS/FAIL): N/A.
- Prompt variant (if applicable): N/A.
- Next experiment (smallest change to try): N/A.
